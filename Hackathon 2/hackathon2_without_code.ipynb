{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hackathon 2\n",
    "\n",
    "In the first exercise, you will develop a model for estimating the cooling/heating load indexes of a building from its characteristics. These indexes serve as reference to adapt the power of air conditioning systems. In the second exercise, you will forecast the hourly electric power consumption in Spain. Electricity being not storable, predicting the electric consumption is a crucial matter for energy producers. You will develop an autoregressive model for this purpose. \n",
    "\n",
    "## Report content\n",
    "\n",
    "‚Ä¢\tYou have to fill in this  jupyter notebook downloadable on the moodle website of the course\n",
    "\n",
    "‚Ä¢\tGrades are granted to the members whose names are in the Jupyter notebook. If your name doesn‚Äôt appear on the top of the notebook, you‚Äôll get a 0, even though you are in a group on Moodle.\n",
    "\n",
    "‚Ä¢\tThe jupyter notebook must be compiled with printed results and next submitted via moodle. The absence of compiled results (or non-printed values) leads to a lower grade.\n",
    "\n",
    "## Report submission\n",
    "\n",
    "‚Ä¢\tThe deadline for submission is reported on the moodle website. Submission after the deadline will not be accepted.\n",
    "\n",
    "‚Ä¢\tTo submit your report, go to the section ‚ÄúAPP‚Äù on Moodle and the subsection ‚ÄúSoumission du rapport‚Äù. You can upload your work there. Once you are sure that it is your final version, click the button ‚ÄúEnvoyer le devoir‚Äù. It is important that you don‚Äôt forget to click on this button ! \n",
    "\n",
    "‚Ä¢\tReports that have not been uploaded through Moodle will not be corrected.\n",
    "\n",
    "## Names and Noma of participants:\n",
    "\n",
    "Part. 1: Martin Gyselinck 19282000\n",
    "\n",
    "Part. 2: Ysaline Paque 18802000\n",
    "\n",
    "Part. 3: Isaline Deckers 21172000\n",
    "\n",
    "Part. 4: Camille D'Hont 21012000\n",
    "\n",
    "Part. 5: Jean de Briey 37941700\n",
    "\n",
    "Part. 6: Guillaume Spronck 48131900\n",
    "\n",
    "## Regression\n",
    "\n",
    "When designing a building, the computation of the heating load (HL) and the cooling load (CL) is required to determine the specifications of the heating and cooling equipment needed to maintain comfortable indoor air conditions. Architects and building designers need information about the characteristics of the building and of the conditioned space. For this reason, we  investigate the effect of eight input variables: (RC), surface area, wall area, roof area, overall height, orientation, glazing area, and glazing area distribution, to determine the output variables HL and CL of residential buildings.\n",
    "The dataset contains eight attributes (or features) and two responses (or outcomes). The aim is to use the eight features to predict each of the two responses.\n",
    "\n",
    "Features: RelativeCompactness, SurfaceArea ,WallArea, RoofArea, OverallHeight, Orientation, GlazingArea, GlazingAreaDistribution.\n",
    "\n",
    "Prediction: HeatingLoad, CoolingLoad\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "1) Report useful statistics for each variables (mean, std, heatmap of correlations,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from datetime import datetime\n",
    "\n",
    "data = pd.read_csv(\"Data_heating_cooling.csv\")\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "variables = list(df.columns)\n",
    "mdf = df.mean()\n",
    "mediandf = df.median()\n",
    "stddf = df.std()\n",
    "quantile_5df = df.quantile(0.05)\n",
    "quantile_95df = df.quantile(0.95)\n",
    "\n",
    "print(df.describe(percentiles=[.05, .95]).round(decimals=3))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(11, 10))  # create 4x2 array of subplots\n",
    "\n",
    "df.boxplot(column='Relative_Compactness', ax=axes[0, 0])\n",
    "df.boxplot(column='Surface_Area', ax=axes[0, 1])\n",
    "df.boxplot(column='Wall_Area', ax=axes[1, 0])\n",
    "df.boxplot(column='Roof_Area', ax=axes[1, 1])\n",
    "df.boxplot(column='Overall_Height', ax=axes[2, 0])\n",
    "df.boxplot(column='Glazing_Area', ax=axes[2,1])\n",
    "fig.suptitle(' Boxplot for each feature ', fontsize=20)\n",
    "plt.show()\n",
    "plt.title(\"Correlation Matrix of the differents features\")\n",
    "sns.heatmap(df.drop(columns=['Heating_Load', 'Cooling_Load']).corr().round(decimals=3), annot=True, cmap='Greens', xticklabels=True)\n",
    "df = pd.get_dummies(df, columns=['Orientation', \"Glazing_Area_Distribution\"], prefix=['Orientation', \"Glazing_Area_Distribution\"], drop_first=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this question we simply extracted the data, dropped the columns HeatingLoad and CoolingLoad since we want to analyze the features. Then we used the describe function available for the dataframe which gives us the parameters for each feature. We then made a boxplot for each numerical variable and finally calculated and printed the correlation between each feature. Finally, we transformed the categorical variables(Orientation and Glazing Area Distribution) using the get Dummies function.\n",
    "\n",
    "We can see that the features: Relative Compactness, Area, Roof Area and Overall Height are strongly correlated to each other and Wall Area is weakly correlated to these features. Orientation is completely decorrelated from all other features. Glazing Area and Glazing Area Distribution are weakly correlated to each other and completely decorrelated to the other features. Note that Orientation and Glazing Area Distribution are dummies variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "2)\tAdd a constant term (intercept) and perform a linear regression of the variable CoolingLoad on all other explanatory variables. Use  the library statsmodels.\n",
    "\n",
    "i. Report the F statistics and R2: interpret them. \n",
    "\n",
    "ii. Analyze the t-statistics and p-values of each coefficient of regression.\n",
    "\n",
    "Are all coefficients significant at 95%? Use the library statsmodels.api. The function OLS accepts pandas dataframe (use .drop() to remove columns).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Cooling_Load']\n",
    "\n",
    "x = sm.add_constant(df.drop(columns=['Cooling_Load', 'Heating_Load', 'Roof_Area'], axis=1))\n",
    "\n",
    "model = sm.OLS(y, x).fit()\n",
    "predictions = model.predict(x)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform a linear regression on the predictive variable CoolingLoad we first dropped the variable Roof Area since it is a linear combination of Surface Area and Wall Area. We then added a constant to all the other variables before computing the linear regression.\n",
    "\n",
    "(i) F-statistic = 471.3 and R-squared = 0.89 which means that the independant variables in the model are significantly related to y (CoolingLoad). In addition we can see that the p-value ('Prob (F-statistic)' -- which is the probability that the null hypothesis for the full model is true) is zero. Since the p-value is zero we reject the null hypothesis, this means, in oder words, that it is obvious that there is a linear relationship. \n",
    "\n",
    "(ii) All coefficients are significant at the 95% level, except for the variables 'Orientation_3', 'Orientation_4' and 'Orientation_5' whose p-value is higher than 5%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "3)\tRemove non-significant coefficients except the intercept (e.g. with  ùõº  5%) and run again the regression. What do you notice when you compare the R2, log-likelihood, AIC and BIC (the AIC and BIC are not explained in the course, search on internet for explanations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(columns=['Cooling_Load', 'Heating_Load', 'Roof_Area', \"Orientation_3\", \"Orientation_4\", \"Orientation_5\"], axis=1)\n",
    "y = df['Cooling_Load']\n",
    "\n",
    "x = sm.add_constant(x)\n",
    "\n",
    "model = sm.OLS(y, x).fit()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After droping 'Orientation_3', 'Orientation_4' and 'Orientation_5' since they are not significant and perform again the linear regression, we can observe that R-squared hasn't change (0.89) which means the model is not better or worse that before. It confirm that the three variables we dropped were unsignificant. The Log-Likelihood is slightly worse (-2.1 from before) but it is normal since we removed three variables. Deleting some predictors variables in a model will always lower the log-likelihood value, even if the deleted predictors variables were not statistically significant. \n",
    "\n",
    "The AIC can be viewed as a measure of the goodness of fit of any estimated statistical model. as the is a type of model selection from a class of parametric models with different numbers of parameters. The values of AIC and BIC attempt to resolve the problem of overfitting. An overfitted model has more parameters that can be justified by the data. Since AIC and BIC are estimator of prefiction error, a lower AIC or BIC value indicates a better fit. In comparison with our first regression, the AIC decreased by 2 and the BIC by 16, which means that the second model is a better fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-regressive model\n",
    "\n",
    "In a paper released early 2019, forecasting in energy markets is identified as one of the highest leverage contribution areas of Machine/Deep Learning toward transitioning to a renewable based electrical infrastructure (see https://arxiv.org/abs/1906.05433).  The file ‚ÄúData_energy_load.csv‚Äù contains 4 years of electrical consumption  for Spain in MW/h.  This was retrieved from ENTSOE a public portal for Transmission Service Operator (TSO) data.\n",
    "\n",
    "File format:\n",
    "\n",
    "Date: dd-mm-yy, Hour : from 0 to 23. Weekday: from 1 (Sunday)  to 7 (Saturday). Load: consumption in MW/h. Lm1: consumption 1h ago, Lm2: consumption 2h ago,‚Ä¶ Lm14: consumption 14h ago\n",
    "\n",
    "1)\tLoad the dataset and convert dates in datetime format (you can use the package datetime). Plot the time series of consumption, what do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as sc\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_energy_load = pd.read_csv(\"Data_energy_load.csv\", sep=\";\", skipinitialspace=True)\n",
    "\n",
    "df = pd.DataFrame(data_energy_load)\n",
    "df[\"Datetime\"] = pd.to_datetime(df[\"Date\"].astype(str) + \"/\" + df[\"Hour\"].astype(str) + \"/\" + (df[\"Weekday\"] - 1).astype(str),\n",
    "                                format=\"%d-%m-%y/%H/%w\")\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(df[\"Datetime\"].to_numpy(), df[\"Load\"].to_numpy())\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Load [MW/h]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that all the loads oscillate widely between 20000 and 40000 MW/h. We can also see that the values do not change significantly according to the different periods of the year. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "2) We will regress the electric consumption (‚ÄòLoad‚Äô) on the following covariates ‚ÄòWeekday‚Äô, ‚ÄòHour‚Äô, ‚ÄòLm1‚Äô, ‚ÄòLm2‚Äô,‚Ä¶,‚ÄôLm14‚Äô. Plot the following graphs:\n",
    "\n",
    "‚Ä¢\tAverage electric consumption per day (y axis) versus weekday (x axis)\n",
    "\n",
    "‚Ä¢\tAverage electric consumption per hour(y axis) versus hour (x axis)\n",
    "\n",
    "Based on these graphs, do you think that it is a good idea to regress linearly the consumption on variables ‚ÄòHour‚Äô and ‚ÄòWeekday‚Äô? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.grid(True)\n",
    "listday = range(1, 8)\n",
    "loadavperday = []\n",
    "for i in listday:\n",
    "    loadavperday.append(np.mean(df[df.get('Weekday') == i].get('Load')))\n",
    "plt.scatter(listday, np.multiply(loadavperday, 24))  #moyenne\n",
    "plt.title(\"Average electric consumption per day versus weekday\")\n",
    "plt.xlabel(\"Weekday\")\n",
    "plt.ylabel(\"Consumption per day [MW/day]\")\n",
    "plt.show()\n",
    "\n",
    "plt.grid(True)\n",
    "listhour = range(24)\n",
    "loadavperhour = []\n",
    "for i in listhour:\n",
    "    loadavperhour.append(np.mean(df[df.get('Hour') == i].get('Load')))\n",
    "plt.scatter(listhour, loadavperhour)  #moyenne\n",
    "plt.title(\"Average electric consumption per hour versus hour\")\n",
    "plt.xlabel(\"Hour [h]\")\n",
    "plt.ylabel(\"Consumption per hour [MW/h]\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first graph represents the average electricity consumption according to the day of the week and the second graph represents the average electricity consumption according the hour of the day.\n",
    "\n",
    "We observe that it is not a good idea to regress linearly on the variables 'Hour' and 'Weekday' for either case. Indeed, the average consumptions vary very strongly according to the hours of the day and according to the days of the week. If we draw a straight line to estimate the average consumption values, we would have very large errors between the estimate and the real values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "3) We will slightly transform the dataset. We first add a constant column for the regression (column of ones). You also convert the variable ‚ÄòHour‚Äô into 23 (and not 24!) binary variables H1,‚Ä¶H23. You next convert the variable ‚ÄòWeekday‚Äô into 6 (and not 7) variables W2,‚Ä¶,W7. Use the command get_dummies(.) from pandas. Why do we remove H0 and W1? Why do we do this conversion?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.insert(len(df.columns) - 1, \"regression\", 1)\n",
    "df = pd.get_dummies(df, columns=[\"Hour\", \"Weekday\"], prefix=[\"Hour\", \"Day\"], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data of hours and weekdays we create two matrices. For each off those matrices we have to remove the first column (H0 and W1). By removing the first column we transform linearly dependent matrices into linearly independent matrices. To resolve this exercise we need matrices that are linearly independant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "4) Split the dataset in\n",
    "\n",
    "‚Ä¢\ta training set with observations from 02-01-15 up to 01-12-2018 (included)\n",
    "\n",
    "‚Ä¢\ta test (or validation) set with observations from 02-12-2018 up to 31-12-2018.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = df[(datetime(2015, 1, 2) <= df[\"Datetime\"]) & (datetime(2018, 12, 1, 23) >= df[\"Datetime\"])]\n",
    "test_set = df[(datetime(2018, 12, 2) <= df[\"Datetime\"]) & (datetime(2018, 12, 31, 23) >= df[\"Datetime\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "If we denote by L(t) the consumption at time t, the model that we want to develop is \n",
    "$$L(t) = cst + \\sum_{k=1}^{14} a_k L(t-k) +\\sum_{k=2}^{7} b_k W_k + \\sum_{k=1}^{23} c_k H_k    $$\n",
    "\n",
    "o\tEstimate this model with statsmodels on the training set. \n",
    "\n",
    "o\tHow would you judge the quality of the predictive model?\n",
    "\n",
    "o\tCompute the Mean Absolute Error (MAE) between predicted and real consumptions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_set.drop([\"Datetime\", \"Load\", \"Date\"], axis=1)\n",
    "Y = training_set[\"Load\"]\n",
    "\n",
    "model = sm.OLS(Y, X).fit()\n",
    "\n",
    "predictions1 = model.predict(X)\n",
    "\n",
    "\n",
    "def computeMAE(Y, y):\n",
    "    sum = 0\n",
    "    for i in range(len(y)):\n",
    "        sum += abs(Y[i] - y[i])\n",
    "    return sum / len(y)\n",
    "\n",
    "\n",
    "MAE = computeMAE(Y, predictions1)\n",
    "print(model.summary())\n",
    "print(\"R2 = \", model.rsquared)\n",
    "print('MAE = ', MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can judge the quality of a predictive model with the R-squared. In our case, it is worth 97.64% which is really good.\n",
    "\n",
    "The MAE on the trainging set is the average error that the model predictions have with respect to their corresponding true values. For our model, the average error between the prediction and the reality is 444.52 MW/h, while the consumptions oscillate between 20 and 30 kMW/h. We can therefore say that we obtain a good value of MAE since the values of our dataset are much higher then the values of the MAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Use this model on the test set to forecast the electric consumption.\n",
    "\n",
    "o\tCompare on a graph, the forecast to  real consumptions on the given period. Plot also the errors of prediction.\n",
    "\n",
    "o\tCompute the MAE on the test set and the R2. Is the forecast reliable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = test_set.drop([\"Datetime\", \"Load\", \"Date\"], axis=1)\n",
    "predictions2 = model.predict(X)\n",
    "real = test_set[\"Load\"]\n",
    "x = np.arange(len(predictions2))\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(x, predictions2, label='predicted consumptions')\n",
    "plt.plot(x, real, label='real consumptions')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('load [MW/h]')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.scatter(x, abs(real-predictions2), label=\"error\", s=4)\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('load [MW/h]')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "MAE2 = computeMAE(real.to_numpy(), predictions2.to_numpy())\n",
    "SSR = np.sum((predictions2 - np.mean(test_set[\"Load\"].to_numpy())) ** 2)\n",
    "SST = np.sum((test_set[\"Load\"] - np.mean(test_set[\"Load\"].to_numpy())) ** 2)\n",
    "print(f\"R2 = {SSR / SST:0.04f}\")\n",
    "print(f'MAE = {MAE2:0.04f}', )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R2 on the test set is 97.04%, it is a little smaller than the one on the training set. But,as for question 4, we can say that it is very good and that the predictions are reliable. \n",
    "The MAE on the test set is 444.82 MW/h, which is still minimal compared to the values on our test set. So we have a very good MAE too.\n",
    "\n",
    "We can conclude that our model estimates very well both a sample over several years and a sample over one month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) We want to check that the average consumption at 11h is significantly different (at 95%) from the average consumption at 13h. Perform a statistical test (explain which stat you use) and report the stat value, the pivot value(s) and the p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "data_energy_load = pd.read_csv(\"Data_energy_load.csv\", sep=\";\", skipinitialspace=True)\n",
    "\n",
    "df = pd.DataFrame(data_energy_load)\n",
    "H11 = df[df.get('Hour') == 11].get('Load')\n",
    "H13 = df[df.get('Hour') == 13].get('Load')\n",
    "\n",
    "M11 = np.mean(H11)\n",
    "M13 = np.mean(H13)\n",
    "\n",
    "STD11 = np.std(H11, ddof=1)\n",
    "STD13 = np.std(H13, ddof=1)\n",
    "\n",
    "LEN11 = len(H11)\n",
    "LEN13 = len(H13)\n",
    "\n",
    "PoolSTD = np.sqrt(((LEN11 - 1) * STD11 * STD11 + (LEN13 - 1) * STD13 * STD13) / (LEN11 + LEN13 - 2))\n",
    "\n",
    "SEM = PoolSTD * np.sqrt((1 / LEN11) + (1 / LEN13))\n",
    "DeFr = LEN11 + LEN13 - 2\n",
    "T = (M11 - M13)\n",
    "\n",
    "t = (M11 - M13) / SEM\n",
    "pval = 2 * sc.t.cdf(-np.abs(t), df=DeFr)\n",
    "t_l, t_u = sc.t.ppf(q=alpha / 2, df=DeFr), sc.t.ppf(q=1 - alpha / 2, df=DeFr)\n",
    "\n",
    "print(\"T-test : \")\n",
    "print(\"H0 : ¬µ11 = ¬µ13\")\n",
    "print(\"H1 : ¬µ11 ‚â† ¬µ13\", end=\"\\n\\n\")\n",
    "\n",
    "print(f\"The t value : {t:0.04f}\")\n",
    "print(f\"Pivots : t_l = {t_l:0.4f} t_u = {t_u:0.4f}\")\n",
    "print(f\"P-value = {pval:0.04f} < 0.05 so we reject the null hypothesis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are testing 2 sample means and the œÉ are unknown, we use this test: $T(X_{11}, X_{13})=\\frac{(\\overline{X}_{11}-\\overline{X}_{13})-0}{S_{pool}*\\sqrt{\\frac{1}{n_{11}}+\\frac{1}{n_{13}}}}$ with hypotheses $H_{0} : ¬µ_{11} = ¬µ_{13}$ and $H_{1} : ¬µ_{11} ‚â† ¬µ_{13}$.\n",
    "\n",
    "We reject $H0$  at the level 5% if:\n",
    "\n",
    "‚Ä¢\twe find $T(X_{11}, X_{13})<t_{n_{11}+n_{13}-2~~0.05/2}$ or if $T(X_{11}, X_{13})>t_{n_{11}+n_{13}-2~~1-0.05/2}$, where \"$t_{n_{11}+n_{13}-2~~Œ±}$\" is the Œ±-percentile of a Student's T.\n",
    "\n",
    "‚Ä¢\tAnd/or we find a p-value smaller than 0,05.\n",
    "\n",
    "We can easily confirm with the results we have that the null hypothesis is rejected. The t value that we we found (-2.1686) is indeed outside the range [t_l t_u] we found(-1.9608 and 1.9608). This is confirmed by the p-value (0.0302) which is smaller than 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) The test of question 6 requires that variances are equal. Test if this assumption holds (95% confidence level). Report  the stat value, the pivot value (s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tobs = STD11 ** 2 / STD13 ** 2\n",
    "fmax = sc.f.ppf(q=0.975, dfn=LEN11 - 1, dfd=LEN13 - 1)\n",
    "fmin = sc.f.ppf(q=0.025, dfn=LEN11 - 1, dfd=LEN13 - 1)\n",
    "pval = 2 * min(sc.f.cdf(Tobs, dfn = LEN11-1, dfd = LEN13-1), 1 - sc.f.cdf(Tobs, dfn = LEN11-1, dfd = LEN13-1))\n",
    "print(f\"Pivots : {fmin:0.4f} {fmax:0.4f}\")\n",
    "print(f\"Tobs : {Tobs:0.4f}\")\n",
    "print(f\"P-value : {pval:0.04f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are testing on 2 variances, we use the test $T(X_{11}, X_{13})=\\frac{{S}_{11}^{2}}{{S}_{13}^{2}}$ with hypotheses $H_{0} : œÉ_{11}^{2} = œÉ_{13}^{2}$ and $H_{1} : œÉ_{11} ‚â† œÉ_{13}$.\n",
    "\n",
    "We reject $H0$  at the level 5% if we find $T(X_{11}, X_{13})<F_{n_{11}+n_{13}-2~~0.05/2}$ or if $T(X_{11}, X_{13})>F_{n_{11}+n_{13}-2~~1-0.05/2}$\n",
    "\n",
    "So we do not reject $H_{0}$ because we found fmin < Tobs < fmax (fmin and fmax are the pivots of our test).\n",
    "\n",
    "We reach the same conclusion with the p-value. Indeed, our p-value is greater than 5%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
